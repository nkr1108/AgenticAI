{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Building A Chatbot***\n",
    "\n",
    "***How to design and implement an LLM-powered chatbot. This chatbot will be able to have a conversation and remember previous interactions.***\n",
    "\n",
    "***Note that this chatbot that we build will only use the language model to have a conversation. There are several other related concepts that you may be looking for:***\n",
    "- *Conversational RAG: Enable a chatbot experience over an external source of data - later*\n",
    "- *Agents: Build a chatbot that can take actions - later*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***Load Env Variables and Keys***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv() ## aloading all the environment variable\n",
    "\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***Import `ChatGroq` - LangChain wrapper***\n",
    "\n",
    "***`ChatGroq` is an integration that lets you use Groq's ultra-fast inference platform (powered by their custom LPM chips) as a chat model inside LangChain. Groq's hardware is designed for deterministic, high-throughput inference, so ChatGroq can deliver low latency responses compared to GPU based providers.***\n",
    "\n",
    "***https://docs.langchain.com/oss/python/integrations/chat/groq***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(profile={'max_input_tokens': 131072, 'max_output_tokens': 8192, 'image_inputs': False, 'audio_inputs': False, 'video_inputs': False, 'image_outputs': False, 'audio_outputs': False, 'video_outputs': False, 'reasoning_output': False, 'tool_calling': True}, client=<groq.resources.chat.completions.Completions object at 0x11d8c1030>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x11d8c0f40>, model_name='llama-3.1-8b-instant', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "model=ChatGroq(model=\"llama-3.1-8b-instant\",groq_api_key=groq_api_key)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Model Parameters***\n",
    "\n",
    "- `max_input_tokens` - takes 131K tokens in single request\n",
    "- `max_output_tokens` - can generate 8k tokens in the response\n",
    "- `image_inputs/audio_inputs/video_inputs`: False - means, its not multimodal, only text\n",
    "- `image_outputs/audio_outputs/video_outputs`: False - cannot generate images, audio or video, only text\n",
    "- `reasoning_output` - no special structured reasoning output model\n",
    "- `tool_calling` - supports structured tool calling (like function calling in OpenAI)\n",
    "- `async_client` - asynchronous version fo the client\n",
    "- `model_kwargs={}` - model configuration - currently empty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***Importing `HumanMessage` class***\n",
    "\n",
    "***We can import these classes when we are invoking the model, to tell the model as who is speaking***\n",
    "\n",
    "- `SystemMessage` - This is more like setting the role/instructions\n",
    "- `HumanMessage` - Contains User Input, actual question\n",
    "- `AIMessage` - Message from AI Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello NK, it's great to meet you. Learning to create AI Agents can be a fascinating and rewarding experience. AI Agents are a fundamental concept in Artificial Intelligence, and understanding them can open doors to a wide range of applications, from robotics and game playing to expert systems and autonomous vehicles.\\n\\nTo get started, what level of experience do you have in programming and AI-related concepts? Are you familiar with any programming languages or AI frameworks, such as Python, TensorFlow, or PyTorch?\\n\\nAlso, are you interested in a specific type of AI Agent, such as:\\n\\n1. Rule-based Agents (e.g., decision trees, expert systems)\\n2. Machine Learning Agents (e.g., neural networks, reinforcement learning)\\n3. Hybrid Agents (combining rule-based and machine learning approaches)\\n4. Other (please specify)\\n\\nLet me know, and I'll be happy to guide you through the process!\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 182, 'prompt_tokens': 47, 'total_tokens': 229, 'completion_time': 0.266685224, 'completion_tokens_details': None, 'prompt_time': 0.00303908, 'prompt_tokens_details': None, 'queue_time': 0.005622034, 'total_time': 0.269724304}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_9ca2574dca', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b2880-ccbd-75e2-b6f6-aa8c9e94e8e3-0', usage_metadata={'input_tokens': 47, 'output_tokens': 182, 'total_tokens': 229})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "model.invoke([HumanMessage(content=\"Hi I am NK and learning how to create AI Agents!\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Output Breakdown***\n",
    "\n",
    "- `AIMessage` - The assistant's reply\n",
    "    - `content` - actual generated text\n",
    "    - `additional_kwargs={}` - empty here, but it is sometimes used for structured outputs like tool calls or function responses. \n",
    "- `response_metadata` - contains multiple items\n",
    "    - `token_usage`\n",
    "        - completion_tokens=282 → The model generated 282 tokens in its answer.\n",
    "        - prompt_tokens=47 → Your input message consumed 47 tokens.\n",
    "        - total_tokens=329 → Combined usage.\n",
    "        - completion_time=0.37s → Time taken to generate the response.\n",
    "        - queue_time=0.006s → Time spent waiting in Groq’s inference queue.\n",
    "        - total_time=0.374s → End‑to‑end latency.\n",
    "    - `usage_metadata`\n",
    "        - 'input_tokens': 47,\n",
    "        - 'output_tokens': 282,\n",
    "        - 'total_tokens': 329"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***Multiple Message when invoking the model***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Your name is NK, and you're learning how to create AI agents.\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 109, 'total_tokens': 125, 'completion_time': 0.01732335, 'completion_tokens_details': None, 'prompt_time': 0.007533376, 'prompt_tokens_details': None, 'queue_time': 0.005171025, 'total_time': 0.024856726}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_9ca2574dca', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b2880-ce5a-7520-8007-59a925df7679-0', usage_metadata={'input_tokens': 109, 'output_tokens': 16, 'total_tokens': 125})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "model.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"Hi I am NK and learning how to create AI Agents!\"),\n",
    "        AIMessage(content=\"Nice to meet you, NK. Learning to create AI agents can be a fascinating and rewarding experience. There are several types of AI agents, including rule-based agents, machine learning agents, and hybrid agents.\\n\"),\n",
    "        HumanMessage(content=\"Hey What's my name and what do I do?\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Important thing to notice***\n",
    "\n",
    "***If you see both the calls to the model, `id` is different for that***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***Message History***\n",
    "\n",
    "***We can use a Message History class to wrap our model and make it stateful. This will keep track of inputs and outputs of the model, and store them in some datastore. Future interactions will then load those messages and pass them into the chain as part of the input. Let's see how to use this!***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Info on Imports***\n",
    "\n",
    "- `ChatMessageHistory` - Provides implementations of chat history storage (in memory, files, databases, Streamlit session state)\n",
    "    - Useful when you want to persist conversations across sessions or store them externally. \n",
    "    - `FileChatMessageHistory` stores messages in a JSON file\n",
    "    - `StreamlitChatMessageHistory` stores them in Streamlit session state\n",
    "- `BaseChatMessageHistory` - Defines the interface for chat history. Any custom history implementation must extend this class\n",
    "- `RunnableWithMessageHistory` - Wrapper that takes any runnable (like a chain or LLM) and automatically manages chat history for it.\n",
    "    - Useful for building chatbots where you want the conversation state to persist without manually handling history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "# Setting up per-session chat history store\n",
    "store = {}\n",
    "\n",
    "# Function to get or create chat history for a session\n",
    "def get_session_history(session_id:str)->BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "with_message_history = RunnableWithMessageHistory(model, get_session_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\":{\"session_id\":\"chat1\"}}\n",
    "\n",
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content = \"Hi I am NK and learning how to create AI Agents!\")],\n",
    "    config = config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello NK.  I'd be happy to help you learn about creating AI Agents. Creating AI Agents involves several key components, including decision-making processes, knowledge representation, and interaction with the environment.\\n\\nThere are several types of AI Agents, including:\\n\\n1. **Simple Reflex Agents**: These agents make decisions based only on the current state of the environment. They do not have memory or the ability to learn from past experiences.\\n2. **Model-Based Agents**: These agents maintain an internal model of the environment, which they use to make predictions and decisions.\\n3. **Goal-Based Agents**: These agents have clear goals and use knowledge and reasoning to achieve those goals.\\n4. **Utility-Based Agents**: These agents make decisions based on a utility function that measures the expected outcome of each action.\\n5. **Learning Agents**: These agents can learn from their experiences and adapt to new situations.\\n6. **Multi-Agent Systems**: These agents interact with each other and the environment, and can lead to complex behaviors.\\n\\nTo create AI Agents, you'll need to consider the following:\\n\\n1. **Perception**: How will the agent gather information about the environment?\\n2. **Action**: How will the agent interact with the environment?\\n3. **Decision-making**: How will the agent make decisions based on the information it has?\\n4. **Knowledge Representation**: How will the agent represent knowledge about the environment and itself?\\n5. **Learning**: How will the agent learn from its experiences?\\n\\nSome popular programming languages for creating AI Agents include:\\n\\n1. **Python**: With libraries like TensorFlow, PyTorch, and Keras, Python is a popular choice for AI development.\\n2. **Java**: With libraries like WEKA and Deeplearning4j, Java is a popular choice for machine learning and AI development.\\n3. **C++**: With libraries like OpenCV and Caffe, C++ is a popular choice for computer vision and deep learning.\\n\\nSome popular frameworks and tools for creating AI Agents include:\\n\\n1. **Reinforcement Learning**: Tools like Gym and PyTorch can help you create reinforcement learning agents.\\n2. **Deep Learning**: Tools like TensorFlow and PyTorch can help you create deep learning models for AI Agents.\\n3. **Agent-Based Modeling**: Tools like NetLogo and Repast can help you create agent-based models.\\n\\nI hope this information helps you get started with creating AI Agents! What specific type of AI Agent are you interested in learning about?\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Your name is NK, and you're learning how to create AI Agents!\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 581, 'total_tokens': 598, 'completion_time': 0.025463836, 'completion_tokens_details': None, 'prompt_time': 0.045098767, 'prompt_tokens_details': None, 'queue_time': 0.010463668, 'total_time': 0.070562603}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_6c980774ec', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b2882-454e-76d0-865d-ce70059f7720-0', usage_metadata={'input_tokens': 581, 'output_tokens': 17, 'total_tokens': 598})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_message_history.invoke(\n",
    "    [HumanMessage(content=\"What's my name and what do I do?\")],\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['chat1'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I don't have any information about you, so I don't know your name. I'm a conversational AI, and our conversation just started. If you'd like to share your name with me, I'd be happy to chat with you.\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## change the config-->session id\n",
    "config1 = {\"configurable\":{\"session_id\":\"chat2\"}}\n",
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Whats my name\")],\n",
    "    config=config1\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"It's nice to meet you, John. How's your day going so far?\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response=with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Hey My name is John\")],\n",
    "    config=config1\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your name is John.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response=with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Whats my name\")],\n",
    "    config=config1\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session ID: chat1\n",
      "Chat History:\n",
      "- content='Hi I am NK and learning how to create AI Agents!' additional_kwargs={} response_metadata={}\n",
      "- content=\"Hello NK.  I'd be happy to help you learn about creating AI Agents. Creating AI Agents involves several key components, including decision-making processes, knowledge representation, and interaction with the environment.\\n\\nThere are several types of AI Agents, including:\\n\\n1. **Simple Reflex Agents**: These agents make decisions based only on the current state of the environment. They do not have memory or the ability to learn from past experiences.\\n2. **Model-Based Agents**: These agents maintain an internal model of the environment, which they use to make predictions and decisions.\\n3. **Goal-Based Agents**: These agents have clear goals and use knowledge and reasoning to achieve those goals.\\n4. **Utility-Based Agents**: These agents make decisions based on a utility function that measures the expected outcome of each action.\\n5. **Learning Agents**: These agents can learn from their experiences and adapt to new situations.\\n6. **Multi-Agent Systems**: These agents interact with each other and the environment, and can lead to complex behaviors.\\n\\nTo create AI Agents, you'll need to consider the following:\\n\\n1. **Perception**: How will the agent gather information about the environment?\\n2. **Action**: How will the agent interact with the environment?\\n3. **Decision-making**: How will the agent make decisions based on the information it has?\\n4. **Knowledge Representation**: How will the agent represent knowledge about the environment and itself?\\n5. **Learning**: How will the agent learn from its experiences?\\n\\nSome popular programming languages for creating AI Agents include:\\n\\n1. **Python**: With libraries like TensorFlow, PyTorch, and Keras, Python is a popular choice for AI development.\\n2. **Java**: With libraries like WEKA and Deeplearning4j, Java is a popular choice for machine learning and AI development.\\n3. **C++**: With libraries like OpenCV and Caffe, C++ is a popular choice for computer vision and deep learning.\\n\\nSome popular frameworks and tools for creating AI Agents include:\\n\\n1. **Reinforcement Learning**: Tools like Gym and PyTorch can help you create reinforcement learning agents.\\n2. **Deep Learning**: Tools like TensorFlow and PyTorch can help you create deep learning models for AI Agents.\\n3. **Agent-Based Modeling**: Tools like NetLogo and Repast can help you create agent-based models.\\n\\nI hope this information helps you get started with creating AI Agents! What specific type of AI Agent are you interested in learning about?\" additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 496, 'prompt_tokens': 47, 'total_tokens': 543, 'completion_time': 0.635393114, 'completion_tokens_details': None, 'prompt_time': 0.003115343, 'prompt_tokens_details': None, 'queue_time': 0.005416361, 'total_time': 0.638508457}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_6c980774ec', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'} id='lc_run--019b2881-d90e-7642-a3e3-417ed48d28ef-0' usage_metadata={'input_tokens': 47, 'output_tokens': 496, 'total_tokens': 543}\n",
      "- content=\"What's my name?\" additional_kwargs={} response_metadata={}\n",
      "- content='Your name is NK.' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 7, 'prompt_tokens': 556, 'total_tokens': 563, 'completion_time': 0.008986587, 'completion_tokens_details': None, 'prompt_time': 0.048675468, 'prompt_tokens_details': None, 'queue_time': 0.009955351, 'total_time': 0.057662055}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_6c980774ec', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'} id='lc_run--019b2882-10bb-7cf3-bc6b-4ddfbf8a96a5-0' usage_metadata={'input_tokens': 556, 'output_tokens': 7, 'total_tokens': 563}\n",
      "- content=\"What's my name and what do I do?\" additional_kwargs={} response_metadata={}\n",
      "- content=\"Your name is NK, and you're learning how to create AI Agents!\" additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 581, 'total_tokens': 598, 'completion_time': 0.025463836, 'completion_tokens_details': None, 'prompt_time': 0.045098767, 'prompt_tokens_details': None, 'queue_time': 0.010463668, 'total_time': 0.070562603}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_6c980774ec', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'} id='lc_run--019b2882-454e-76d0-865d-ce70059f7720-0' usage_metadata={'input_tokens': 581, 'output_tokens': 17, 'total_tokens': 598}\n",
      "\n",
      "Session ID: chat2\n",
      "Chat History:\n",
      "- content='Whats my name' additional_kwargs={} response_metadata={}\n",
      "- content=\"I don't have any information about you, so I don't know your name. I'm a conversational AI, and our conversation just started. If you'd like to share your name with me, I'd be happy to chat with you.\" additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 51, 'prompt_tokens': 38, 'total_tokens': 89, 'completion_time': 0.068681267, 'completion_tokens_details': None, 'prompt_time': 0.002040429, 'prompt_tokens_details': None, 'queue_time': 0.005931755, 'total_time': 0.070721696}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_9ca2574dca', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'} id='lc_run--019b2883-92c3-7f93-b386-f6f131d924f1-0' usage_metadata={'input_tokens': 38, 'output_tokens': 51, 'total_tokens': 89}\n",
      "- content='Hey My name is John' additional_kwargs={} response_metadata={}\n",
      "- content=\"It's nice to meet you, John. How's your day going so far?\" additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 103, 'total_tokens': 121, 'completion_time': 0.021316021, 'completion_tokens_details': None, 'prompt_time': 0.006463817, 'prompt_tokens_details': None, 'queue_time': 0.006849613, 'total_time': 0.027779838}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_9ca2574dca', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'} id='lc_run--019b2883-d561-7fa3-94ed-bf9b2d57c77c-0' usage_metadata={'input_tokens': 103, 'output_tokens': 18, 'total_tokens': 121}\n",
      "- content='Whats my name' additional_kwargs={} response_metadata={}\n",
      "- content='Your name is John.' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 6, 'prompt_tokens': 133, 'total_tokens': 139, 'completion_time': 0.010349188, 'completion_tokens_details': None, 'prompt_time': 0.011828064, 'prompt_tokens_details': None, 'queue_time': 0.006604163, 'total_time': 0.022177252}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_9ca2574dca', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'} id='lc_run--019b2883-e833-7292-bc46-a5e4d6508b02-0' usage_metadata={'input_tokens': 133, 'output_tokens': 6, 'total_tokens': 139}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print the dictionary store\n",
    "for key, value in store.items():\n",
    "    print(f\"Session ID: {key}\")\n",
    "    print(\"Chat History:\")\n",
    "    for message in value.messages:\n",
    "        print(f\"- {message}\")\n",
    "    print()  # Add an empty line between sessions   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***Prompt templates***\n",
    "\n",
    "***Prompt Templates help to turn raw user information into a format that the LLM can work with. In this case, the raw user input is just a message, which we are passing to the LLM. Let's now make that a bit more complicated. First, let's add in a system message with some custom instructions (but still taking messages as input). Next, we'll add in more input besides just the messages.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `ChatPromptTemplate` - Template for constructing prompts made up of multiple chat messages (System, Human, AI)\n",
    "- `MessagesPlaceholder` - Special Placeholder that lets you inject a list of messages (like past conversation history) into the prompt dynamically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are a helpful assistant.Answer all the question to the best of your ability\"),\n",
    "        MessagesPlaceholder(variable_name=\"messages\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "# by MessagesPlaceholder, you can pass the chat history dynamically at runtime.\n",
    "# messages should be given with a list of HumanMessage / AI Message - key value pair\n",
    "\n",
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello NK. It's nice to meet you. Is there something I can help you with or would you like to chat?\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 26, 'prompt_tokens': 56, 'total_tokens': 82, 'completion_time': 0.026865983, 'completion_tokens_details': None, 'prompt_time': 0.004668943, 'prompt_tokens_details': None, 'queue_time': 0.06880162, 'total_time': 0.031534926}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_6b5c123dd9', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b28da-1801-7923-b5aa-b73c752e61d5-0', usage_metadata={'input_tokens': 56, 'output_tokens': 26, 'total_tokens': 82})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"messages\":[HumanMessage(content=\"Hi My name is NK\")]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history = RunnableWithMessageHistory(chain, get_session_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Nice to meet you, NK. How are you today? Is there anything I can help you with?', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 22, 'prompt_tokens': 56, 'total_tokens': 78, 'completion_time': 0.034498413, 'completion_tokens_details': None, 'prompt_time': 0.003009483, 'prompt_tokens_details': None, 'queue_time': 0.005380644, 'total_time': 0.037507896}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_29e590f0c0', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b28da-f7e3-7091-8734-e6787c7695fc-0', usage_metadata={'input_tokens': 56, 'output_tokens': 22, 'total_tokens': 78})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {\"configurable\": {\"session_id\": \"chat3\"}}\n",
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Hi My name is NK\")],\n",
    "    config = config\n",
    ")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your name is NK.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"What's my name?\")],\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are a helpful assistant.Answer all the question to the best of your ability in {language}\"),\n",
    "        MessagesPlaceholder(variable_name=\"messages\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'नमस्ते NK, मैं आपकी सहायता के लिए यहाँ हूँ। क्या मुझे कुछ बताना है या आपके पास कोई प्रश्न है?'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chain.invoke({\"messages\":[HumanMessage(content=\"Hi My name is NK\")],\"language\":\"Hindi\"})\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now wrap this more complicated chain in a Message History class. This time, because there are multiple keys in the input, we need to specify the correct key to use to save the chat history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"messages\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'नमस्ते NK, मैं आपकी मदद करने के लिए तैयार हूँ। क्या मैं आपकी किसी विशिष्ट समस्या या प्रश्न का समाधान करने में आपकी सहायता कर सकता हूँ?'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {\"configurable\": {\"session_id\": \"chat4\"}}\n",
    "repsonse=with_message_history.invoke(\n",
    "    {'messages': [HumanMessage(content=\"Hi,I am NK\")],\"language\":\"Hindi\"},\n",
    "    config=config\n",
    ")\n",
    "repsonse.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = with_message_history.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"whats my name?\")], \"language\": \"Hindi\"},\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'तुम्हारा नाम NK है।'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***Managing the Conversation History***\n",
    "\n",
    "- One important concept to understand when building chatbots is how to manage conversation history. If left unmanaged, the list of messages will grow unbounded and potentially overflow the context window of the LLM. Therefore, it is important to add a step that limits the size of the messages you are passing in.\n",
    "- `trim_messages` helper to reduce how many messages we're sending to the model. The trimmer allows us to specify how many tokens we want to keep, along with other parameters like if we want to always keep the system message and whether to allow partial messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d95bb196fd0f4812ad0b75b7d75cb4b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2305645e32194fbf868d4ece6686d423",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50639aaded1c4d31bd444c94f12e262c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0df045be890497b830c984480796ed3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f87394463904092831aa7f24c11a7aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content=\"you're a good assistant\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='I like vanilla ice cream', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='nice', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='whats 2 + 2', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='4', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='thanks', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='no problem!', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='having fun?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='yes!', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage, trim_messages\n",
    "trimmer = trim_messages(\n",
    "    max_tokens=45,\n",
    "    strategy=\"last\",\n",
    "    token_counter=model,\n",
    "    include_system=True,\n",
    "    allow_partial=False,\n",
    "    start_on=\"human\"\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"you're a good assistant\"),\n",
    "    HumanMessage(content=\"hi! I'm bob\"),\n",
    "    AIMessage(content=\"hi!\"),\n",
    "    HumanMessage(content=\"I like vanilla ice cream\"),\n",
    "    AIMessage(content=\"nice\"),\n",
    "    HumanMessage(content=\"whats 2 + 2\"),\n",
    "    AIMessage(content=\"4\"),\n",
    "    HumanMessage(content=\"thanks\"),\n",
    "    AIMessage(content=\"no problem!\"),\n",
    "    HumanMessage(content=\"having fun?\"),\n",
    "    AIMessage(content=\"yes!\"),\n",
    "]\n",
    "\n",
    "trimmer.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm not aware of your preferences, but I can help you explore different types of ice cream if you'd like. Do you have a favorite flavor or type of ice cream?\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "chain = (\n",
    "    RunnablePassthrough.assign(messages=itemgetter(\"messages\") | trimmer)\n",
    "    | prompt\n",
    "    | model\n",
    "    \n",
    ")\n",
    "\n",
    "response = chain.invoke(\n",
    "    {\n",
    "    \"messages\":messages + [HumanMessage(content=\"What ice cream do i like\")],\n",
    "    \"language\":\"English\"\n",
    "    }\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You asked the math problem 2 + 2.'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chain.invoke(\n",
    "    {\n",
    "        \"messages\": messages + [HumanMessage(content=\"what math problem did i ask\")],\n",
    "        \"language\": \"English\",\n",
    "    }\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lets wrap this in the MEssage History\n",
    "with_message_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"messages\",\n",
    ")\n",
    "config={\"configurable\":{\"session_id\":\"chat5\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"You didn't tell me your name, so I don't know what it is. Would you like to share?\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    {\n",
    "        \"messages\": messages + [HumanMessage(content=\"whats my name?\")],\n",
    "        \"language\": \"English\",\n",
    "    },\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"You didn't ask a math problem yet. Our conversation has just started. What would you like to ask or discuss?\""
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    {\n",
    "        \"messages\": [HumanMessage(content=\"what math problem did i ask?\")],\n",
    "        \"language\": \"English\",\n",
    "    },\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "response.content"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agenticai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
