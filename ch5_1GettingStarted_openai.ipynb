{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### ***Getting started With Langchain And Open AI***\n",
    "---\n",
    "\n",
    "- ***Get setup with LangChain, LangSmith and LangServe***\n",
    "- ***Use the most basic and common components of LangChain: prompt templates, models, and output parsers.***\n",
    "- ***Build a simple application with LangChain***\n",
    "- ***Trace your application with LangSmith (Debug)***\n",
    "- ***Serve your application with LangServe (Production Ready)***\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Steps***\n",
    "- ***Create an Account in LangChain - `done`***\n",
    "- ***Obtain Langchain API KEY - `done`***\n",
    "- ***Obtain OPENAI Key - `done`***\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Environment Setup for LangChain and OpenAI***\n",
    "\n",
    "***This script configures environment variables for using **OpenAI** and **LangChain** with tracking enabled.  \n",
    "It relies on the `.env` file to securely store API keys and project settings.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ['OPENAI_API_KEY']=os.getenv(\"OPENAI_API_KEY\")\n",
    "## Langsmith Tracking\n",
    "os.environ[\"LANGCHAIN_API_KEY\"]=os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"]=os.getenv(\"LANGCHAIN_PROJECT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Loads `ChatOpenAI` and creates an instance of `ChatOpenAI` with the given model `gpt-4o`***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "profile={'max_input_tokens': 128000, 'max_output_tokens': 16384, 'image_inputs': True, 'audio_inputs': False, 'video_inputs': False, 'image_outputs': False, 'audio_outputs': False, 'video_outputs': False, 'reasoning_output': False, 'tool_calling': True, 'structured_output': True, 'image_url_inputs': True, 'pdf_inputs': True, 'pdf_tool_message': True, 'image_tool_message': True, 'tool_choice': True} client=<openai.resources.chat.completions.completions.Completions object at 0x10c348220> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x16237bd90> root_client=<openai.OpenAI object at 0x10c34b7f0> root_async_client=<openai.AsyncOpenAI object at 0x16237bcd0> model_name='gpt-4o' model_kwargs={} openai_api_key=SecretStr('**********') stream_usage=True\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI # ChatOpenAI class is LangChain wrapper for OpenAI's chat models\n",
    "llm = ChatOpenAI(model=\"gpt-4o\") # creates an instance of ChatOpenAI with the specified model\n",
    "print(llm) # displays the configuration of the ChatOpenAI instance like model name, temperature, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Input and get response form LLM***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Input and get response form LLM\n",
    "\n",
    "result = llm.invoke(\"What is generative AI?\") # invokes the LLM with the given prompt and stores the response in result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"Generative AI refers to a subset of artificial intelligence techniques designed to generate new content. This can include text, images, audio, video, and other data types. Unlike traditional AI, which is often used for classification, prediction, or decision-making tasks, generative AI focuses on creating data that mimics or extends real-world content.\\n\\nThe core technology behind many generative AI applications is neural networks, particularly deep learning models such as Generative Adversarial Networks (GANs), Variational Autoencoders (VAEs), and autoregressive models like the Transformer architecture. These models learn patterns and structures from input data during training and can generate new, original outputs that share similar characteristics with the training data.\\n\\nApplications of generative AI are diverse and expanding, including:\\n\\n1. **Text Generation**: Tools like OpenAI's GPT (Generative Pre-trained Transformer) models create human-like text for chatbots, content creation, and more.\\n\\n2. **Image and Video Creation**: GANs can produce realistic images and videos, used in art, video game design, and even creating digital avatars.\\n\\n3. **Music and Audio Synthesis**: Generative models can compose music or generate realistic voice content for dialogue in games or virtual assistants.\\n\\n4. **Design and Art**: AI can assist in creating new designs, logos, or even entire virtual environments.\\n\\n5. **Data Augmentation**: Creating synthetic data to augment training datasets, especially when collecting real-world data is expensive or impractical.\\n\\nOverall, generative AI offers powerful tools for innovation across various fields, raising both opportunities and challenges, especially regarding creativity, ethics, and copyright considerations.\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 331, 'prompt_tokens': 13, 'total_tokens': 344, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_83554c687e', 'id': 'chatcmpl-CmQi7mcvp5dFEAJ3OpcGTwdlduONA', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--019b1971-eb08-7c61-a5dc-be02e1b30734-0' usage_metadata={'input_tokens': 13, 'output_tokens': 331, 'total_tokens': 344, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Chatprompt Template***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an expert AI Engineer. Provide me answers based on the questions'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate # Importing ChatPromptTemplate from langchain_core.prompts module\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are an expert AI Engineer. Provide me answers based on the questions\"),\n",
    "        (\"user\",\"{input}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"Langsmith, introduced by LangChain, is a platform designed to optimize the creation, debugging, and monitoring of applications that are built using language models. It offers developers the ability to manage and refine their applications by providing key tools for evaluating outputs, understanding performance, and ensuring quality. Langsmith supports end-to-end testing and monitoring, making it easier for developers to fine-tune their language model applications for better reliability and efficiency. The platform's emphasis on observability and debugging helps developers address common challenges in the lifecycle of language model applications.\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 107, 'prompt_tokens': 33, 'total_tokens': 140, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_83554c687e', 'id': 'chatcmpl-CmQiS8iH8YUonIU9awe6RXqbaCnTU', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--019b1972-39c2-7390-9f9c-56b00e1ed013-0' usage_metadata={'input_tokens': 33, 'output_tokens': 107, 'total_tokens': 140, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "## chain \n",
    "chain = prompt | llm\n",
    "\n",
    "response = chain.invoke({\"input\":\"Can you tell me about Langsmith?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.messages.ai.AIMessage"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***String Output Parser***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Langsmith is a suite of tools designed for building, monitoring, and improving applications powered by large language models (LLMs). It offers features to help developers create, evaluate, and refine the performance of LLMs, making it easier to iterate on and optimize applications that use these models. Langsmith aims to provide insights and metrics that can guide developers in enhancing the effectiveness and efficiency of their LLM applications. This includes tracking how models perform on various tasks and understanding user interactions, which are crucial for maintaining high-quality language model experiences.\n"
     ]
    }
   ],
   "source": [
    "## stroutput Parser\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "output_parser = StrOutputParser()\n",
    "chain = prompt | llm | output_parser\n",
    "\n",
    "response = chain.invoke({\"input\":\"Can you tell me about Langsmith?\"})\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agenticai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
